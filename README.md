# demo-spanish-bot

Выбранный мной метод обучения — это дообучение модели LLaMA-3.1-8B (поддерживающей испанский) с использованием адаптера QLoRA на синтезированном (переведенном) датасете. Датасет был переведен с английского ([датасет](https://huggingface.co/datasets/bavard/personachat_truecased)), он содержит простые диалоги между двумя собеседниками на разнообразные темы. 

## Полезные ссылки
[переведенный датасет](https://huggingface.co/datasets/alinatl/personachat_truecased-spanish)

[метрики wandb](https://wandb.ai/alinatl/huggingface/runs/8i4jo95c/workspace?nw=nwuseralinatl)

[обученный адаптер на HF](https://huggingface.co/alinatl/Llama-3.1-8B-Instruct-qlora-spanish)


## Почему QLoRa:
- QLoRA обеспечивает эффективную адаптацию крупных языковых моделей к конкретным задачам или доменам. В данном случае улучшает перформанс для испанского языка и модет быть в дальнейшем использовано для дообучения необходимому диалекту/стилю общения.

- Благодаря сочетанию квантования (снижение точности модели до 4 или 8 бит) и адаптации с пониженным ранком, QLoRA значительно снижает требования к памяти и вычислительным ресурсам при дообучении. Это позволяет использовать более крупные модели (такие как LLaMA 8B), даже при ограниченных ресурсах (в моем случае натренировать в колаб). 

P.S. Файлы .ipynb некорректно отображаются на GitHub из-за внутренней ошибки, связанной с виджетами. Для устранения проблемы необходимо удалить все виджеты. При этом в Jupyter ноутбуки продолжают отображаться корректно. Я решила для демонстративности не удалять виджеты, поэтому для просмотра нужно будет скачать файлы [обсуждение](https://github.com/orgs/community/discussions/155944)


